{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e661dd9d",
   "metadata": {},
   "source": [
    "## 语义切分 （Semantic Chunking）\n",
    "以下代码参考自LlamaIndex的`llama-index-core/llama_idex/core/node_parser/text/semantic_splitter.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e75c33",
   "metadata": {},
   "source": [
    "### 1. 文本提取 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75dc23ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 22\n",
      "First page text:\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
      "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
      "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
      "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
      "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
      "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\n",
      "R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\n",
      "research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n",
      "(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based o...\n"
     ]
    }
   ],
   "source": [
    "import pypdf\n",
    "\n",
    "pdfreader = pypdf.PdfReader('../docs/deepseek-r1.pdf')\n",
    "num_pages = len(pdfreader.pages)\n",
    "print(f'Number of pages: {num_pages}')\n",
    "\n",
    "page = pdfreader.pages[0]\n",
    "text = page.extract_text()\n",
    "print(f'First page text:\\n{text[:1000]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e99e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pages text:\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
      "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
      "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
      "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
      "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
      "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\n",
      "R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\n",
      "research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n",
      "(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based o...\n"
     ]
    }
   ],
   "source": [
    "all_pages_text = \"\".join([page.extract_text() for page in pdfreader.pages])\n",
    "print(f'All pages text:\\n{all_pages_text[:1000]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8021c67",
   "metadata": {},
   "source": [
    "### 2. 句子切分\n",
    "\n",
    "使用正则表达式来切分句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96852b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
      "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n"
     ]
    }
   ],
   "source": [
    "CHUNKING_REGEX = r'(?<=[.?!])\\s+'\n",
    "\n",
    "import re\n",
    "\n",
    "sentences = re.split(CHUNKING_REGEX, all_pages_text)\n",
    "print(sentences[0])\n",
    "print('\\n' + sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125ff10",
   "metadata": {},
   "source": [
    "### 3. 句子组合\n",
    "\n",
    "通过将每个句子与其前后的句子组合，以提供更广泛的上下文。例如，将句子1、2、3和2组合在一起。在本实现中默认使用前后两个句子作为一个组合，这样可以考独立地虑每个句子间的相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b781bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "BUFFER_SIZE = 1\n",
    "\n",
    "def build_sentence_groups(sentences: List[str], buffer_size: int=BUFFER_SIZE) -> List[str]:\n",
    "    \"\"\" \n",
    "    Create a buffer by combining each sentence with its previous and next sentence \n",
    "    to provide a wider context. \n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): The list of sentences to be combined\n",
    "        buffer_size (int, optional): The size of the buffer. \n",
    "                                     Defaults to 1 to consider each sentence individually.\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: The list of combined sentences\n",
    "    \"\"\"\n",
    "\n",
    "    combined_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = \"\"\n",
    "\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]\n",
    "        \n",
    "        combined_sentence += sentences[i] + ' '\n",
    "\n",
    "        for j in range(i + 1, i + buffer_size + 1):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]\n",
    "\n",
    "        combined_sentences.append(combined_sentence)\n",
    "    return combined_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6356fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.  DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
      "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n"
     ]
    }
   ],
   "source": [
    "combined_sentences = build_sentence_groups(sentences)\n",
    "print(combined_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3a6b7",
   "metadata": {},
   "source": [
    "### 4. 文本嵌入\n",
    "使用Qwen3-Embedding-0.6B-Q8_0将文本转换为向量，以便计算余弦相似度。使用LMStudio作为推理引擎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "097dec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_text_embedding(text: str, model: str=\"model-identifier\") -> List[float]:\n",
    "   \"\"\" \n",
    "    Get the embedding of the text using Qwen3-Embedding-0.6B-Q8_0.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be embedded\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The embedding of the text\n",
    "    \"\"\"\n",
    "   \n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "def get_text_embedding_batch(texts: list[str], model: str=\"model-identifier\") -> List[List[float]]:\n",
    "   \"\"\"\n",
    "   Get the text embeddings of the whole text batch\n",
    "\n",
    "   Args:\n",
    "       texts (list[str]): The text batch to be embedded\n",
    "\n",
    "   Returns:\n",
    "       list[list[float]]: The text embeddings of the whole text batch\n",
    "   \"\"\"\n",
    "\n",
    "   return [get_text_embedding(text, model) for text in tqdm(texts, desc=\"Text Embedding...\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1b8d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text Embedding...: 100%|██████████| 1230/1230 [00:39<00:00, 31.14it/s]\n"
     ]
    }
   ],
   "source": [
    "embs = get_text_embedding_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "863a0771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of embeddings: 1230\n",
      "The size of embeddings: 1024\n",
      "\n",
      "The initial 10 entries in the first embedding: [0.01764315739274025, -0.007887820713222027, -0.004054180346429348, -0.025975460186600685, -0.01984322816133499, -0.008266817778348923, -0.04062263295054436, 0.0263111162930727, -0.03725064545869827, 0.020719148218631744]\n",
      "\n",
      "The initial 10 entries in the second embedding: [0.001355705433525145, -0.0073656318709254265, -0.005453282967209816, -0.031235404312610626, -0.012917835265398026, 0.027917563915252686, -0.0356772281229496, 0.025879625231027603, -0.04084746539592743, 0.04088159278035164]\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of embeddings: {len(embs)}')\n",
    "print(f'The size of embeddings: {len(embs[0])}')\n",
    "print(f'\\nThe initial 10 entries in the first embedding: {embs[0][:10]}')\n",
    "print(f'\\nThe initial 10 entries in the second embedding: {embs[1][:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ad631",
   "metadata": {},
   "source": [
    "### 5. 计算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ef2ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_similarity(emb1: List[float], emb2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Get the similarity between two embeddings using the cosine similarity\n",
    "\n",
    "    Args:\n",
    "        emb1 (list[float]): The first embedding\n",
    "        emb2 (list[float]): The second embedding\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity between the two embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(emb1, List) and isinstance(emb2, List):\n",
    "        emb1 = np.array(emb1)\n",
    "        emb2 = np.array(emb2)\n",
    "\n",
    "    product = np.dot(emb1, emb2)\n",
    "    norm = np.linalg.norm(emb1) * np.linalg.norm(emb2)\n",
    "    return product / norm\n",
    "\n",
    "def calculate_distances_between_embeddings(embs: List[List[float]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate the distances between two consecutive embeddings in the list\n",
    "\n",
    "    Args:\n",
    "        embs (list[list[float]]): The list of embeddings to be compared\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The list of distances between all pairs of consecutive embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    distances = []\n",
    "    for i in tqdm(range(len(embs) - 1), desc=\"Calculating distances between pairs of embeddings...\"):\n",
    "        emb_current = embs[i]\n",
    "        emb_next = embs[i + 1]\n",
    "\n",
    "        similarity = get_similarity(emb_current, emb_next)\n",
    "        distance = 1 - similarity # using the 1 - cosine similarity to measure distance\n",
    "\n",
    "        distances.append(distance)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f2be6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating distances between pairs of embeddings...: 100%|██████████| 1229/1229 [00:00<00:00, 11123.12it/s]\n"
     ]
    }
   ],
   "source": [
    "embs_distances = calculate_distances_between_embeddings(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6bc2b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of distances calculated: 1229\n",
      "\n",
      "The distance between the first and the second embedding is 0.15388124054777919\n",
      "\n",
      "The distance between the second and the third embedding is 0.14347669159212484\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of distances calculated: {len(embs_distances)}')\n",
    "print(f'\\nThe distance between the first and the second embedding is {embs_distances[0]}')\n",
    "print(f'\\nThe distance between the second and the third embedding is {embs_distances[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e4251",
   "metadata": {},
   "source": [
    "### 6. 基于相似度合并文本块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f1106a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_chunk(combined_sentences: List[str], \n",
    "                     embs_distances: List[float], \n",
    "                     threshold: float) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build text chunks based on the combined sentences and the distances between embeddings.\n",
    "    If the distance between two consecutive embeddings is greater than a threshold,\n",
    "    it means that the two sentences are not similar enough to be merged into one chunk.\n",
    "    Therefore, we need to split the combined sentences into separate chunks.\n",
    "    \n",
    "    Args:\n",
    "        combined_sentences (List[str]): The combined sentences.\n",
    "        embs_distances (List[float]): The distances between embeddings.\n",
    "        threshold (float): The threshold for determining whether two sentences are similar enough to be merged into one chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The text chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []\n",
    "    if len(embs_distances) > 0:\n",
    "        breakpoint_distance_threshold = np.percentile(embs_distances, threshold)\n",
    "\n",
    "        indices_above_threshold = [\n",
    "            i for i, x in enumerate(embs_distances) if x > breakpoint_distance_threshold\n",
    "        ]\n",
    "\n",
    "        start_idx = 0\n",
    "        for idx in tqdm(indices_above_threshold, desc=\"Building Text Chunks...\"):\n",
    "            group = combined_sentences[start_idx : idx + 1]\n",
    "            combined_text = \" \".join(group)\n",
    "            chunks.append(combined_text)\n",
    "\n",
    "            start_idx = idx + 1\n",
    "\n",
    "        if start_idx < len(combined_sentences):\n",
    "            combined_text = \" \".join(combined_sentences[start_idx:])\n",
    "            chunks.append(combined_text)\n",
    "    else:\n",
    "        chunks.append(\" \".join(combined_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "814b3caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Text Chunks...: 100%|██████████| 62/62 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 95\n",
    "\n",
    "text_chunks = build_text_chunk(combined_sentences, embs_distances, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1a49f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Chunks: 63\n",
      "\n",
      "The chunk: \n",
      "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
      "reasoning behaviors.However, it encounters challenges such as poor readability, and language\n",
      "mixing.  To address these issues and further enhance reasoning performance, we introduce\n",
      "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of Chunks: {len(text_chunks)}')\n",
    "print(f'\\nThe chunk: \\n{text_chunks[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab34aaf",
   "metadata": {},
   "source": [
    "自此完成了文本地语义切分，并构造了文本块\n",
    "\n",
    "为了方便以后调用，以上代码会被封装到一个类中，并添加到`nanoidx/semantic_splitter.py`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe69693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
